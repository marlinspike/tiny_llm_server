DEFAULT_MODEL=tinyllama
OUTPUT_TOKENS=2000
PORT=6001
TEMPERATURE=0.5
LLM_API_URL=http://localhost:6001/predict
WEB_SERVER_PORT=8000