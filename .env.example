DEFAULT_MODEL=tinyllama
OUTPUT_TOKENS=250
PORT=6001
TEMPERATURE=0.5
LLM_API_URL=http://localhost:6001/predict
WEB_SERVER_PORT=8000
LLM_SERVER_PORT=6001
LOG_QUERIES=true
LOG_LEVEL=info